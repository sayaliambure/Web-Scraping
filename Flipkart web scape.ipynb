{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a66119",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (from requests) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages (from requests) (2022.12.7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution - (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -andas (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -pencv-python-headless (c:\\users\\sayali ambure\\anaconda3\\lib\\site-packages)\n",
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install bs4\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7c4298d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79e1067",
   "metadata": {},
   "source": [
    "We need to send request from our local PC to amazon website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "01c16ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://www.flipkart.com/clothing-and-accessories/dresses-and-gown/dress/women-dress/pr?sid=clo,odx,maj,jhy&otracker=categorytree&otracker=nmenu_sub_Women_0_Dresses\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "3e744a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for request\n",
    "HEADERS = ({\"User-Agent\":'', 'Accept-Language': 'en-US, en;q=0.5'})  \n",
    "# add your user-agent by navigatin to https://www.whatismybrowser.com/\n",
    "# And then going to detect my settings tab\n",
    "# Under \"Detect Web Browser\", click \"What is my user agent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c2804ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP request\n",
    "webpage = requests.get(URL, headers=HEADERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d92a0be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "55ea82f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(webpage.content)\n",
    "# convert it into proper html format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c2c25a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Soup object containing all data\n",
    "# converts the requested page into html format\n",
    "soup = BeautifulSoup(webpage.content, 'html.parser')\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "9b74126e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch links as list of tag objects\n",
    "links = soup.find_all('a', attrs={'class': 'WKTcLC'})\n",
    "# links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3b537b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = links[0].get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1c336506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://flipkart.com/sassafras-women-high-low-brown-dress/p/itmb3242bdb9507e?pid=DREFJ3FK7XS3GZEX&lid=LSTDREFJ3FK7XS3GZEXW490CO&marketplace=FLIPKART&store=clo%2Fodx%2Fmaj%2Fjhy&srno=b_1_1&otracker=browse&fm=organic&iid=059fcb03-ecb5-4aeb-a554-7ef32bafdaac.DREFJ3FK7XS3GZEX.SEARCH&ppt=None&ppn=None&ssid=28mlgwao9c0000001714405466290'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_list = 'https://flipkart.com' + link\n",
    "product_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d7a3c334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_webpage = requests.get(product_list, headers=HEADERS)\n",
    "new_webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a01cfed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_soup = BeautifulSoup(new_webpage.content, 'html.parser')\n",
    "# new_soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "36367a95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Women High Low Brown Dress'"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Name of product\n",
    "new_soup.find('span', attrs={'class':'VU-ZEz'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "db0339d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â‚¹859'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# price\n",
    "new_soup.find('div', attrs={'class':'Nx9bqj CxhGGd'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "4a272b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating\n",
    "new_soup.find('div', attrs={'class':'XQDdHH _1Quie7'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d50f4382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4,876 ratings and 606 reviews'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rating and review count\n",
    "new_soup.find(\"span\", attrs={'class':'Wphh3N'}).string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d9d1e92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'57% off'"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# offer %\n",
    "new_soup.find(\"div\", attrs={'class':'UkUFwK WW8yVX dB67CR'}).string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8ddbc6a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# flipkart assured\n",
    "assured = new_soup.find(\"span\", attrs={'class':'+N9xME hxqV2A'})\n",
    "if assured:\n",
    "    print(\"Yes\")\n",
    "else:\n",
    "    print(\"No\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6db9187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c2197590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract Product Title\n",
    "def get_title(soup):\n",
    "\n",
    "    try:\n",
    "        # Outer Tag Object\n",
    "        title = soup.find(\"span\", attrs={'class':'VU-ZEz'})\n",
    "        \n",
    "        # Inner NavigatableString Object\n",
    "        title_value = title.text\n",
    "\n",
    "        # Title as a string value\n",
    "        title_string = title_value.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        title_string = \"\"\n",
    "\n",
    "    return title_string\n",
    "\n",
    "# Function to extract Product Price\n",
    "def get_price(soup):\n",
    "\n",
    "    try:\n",
    "        price = soup.find('div', attrs={'class':'Nx9bqj CxhGGd'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "\n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            price = soup.find('div', attrs={'class':'Nx9bqj CxhGGd'}).string.strip()\n",
    "\n",
    "        except:\n",
    "            price = \"\"\n",
    "\n",
    "    return price\n",
    "\n",
    "# Function to extract Product Rating\n",
    "def get_rating(soup):\n",
    "\n",
    "    try:\n",
    "        rating = soup.find('div', attrs={'class':'XQDdHH _1Quie7'}).text\n",
    "    \n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # If there is some deal price\n",
    "            rating = soup.find('div', attrs={'class':'XQDdHH _1Quie7'}).text\n",
    "\n",
    "        except:\n",
    "            rating = \"\"\n",
    "\n",
    "    return rating\n",
    "\n",
    "# Function to extract Number of User Reviews\n",
    "def get_review_count(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"span\", attrs={'class':'Wphh3N'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n",
    "\n",
    "# Function to extract offer percent on prod\n",
    "def get_offer_per(soup):\n",
    "    try:\n",
    "        review_count = soup.find(\"div\", attrs={'class':'UkUFwK WW8yVX dB67CR'}).string.strip()\n",
    "\n",
    "    except AttributeError:\n",
    "        review_count = \"\"\t\n",
    "\n",
    "    return review_count\n",
    "\n",
    "\n",
    "# If product is flipkart assured\n",
    "def flipkart_assured(soup):\n",
    "    try:\n",
    "        assured =  new_soup.find(\"span\", attrs={'class':'+N9xME hxqV2A'})\n",
    "        if assured:    \n",
    "            flipkart_assured = \"Yes\"\n",
    "        else:\n",
    "            flipkart_assured = \"No\"            \n",
    "        \n",
    "    except AttributeError:\n",
    "            flipkart_assured = \"No\"\n",
    "            \n",
    "    return flipkart_assured\n",
    "\n",
    "\n",
    "# Function to extract Availability Status\n",
    "# def get_availability(soup):\n",
    "#     try:\n",
    "#         available = soup.find(\"div\", attrs={'id':'availability'})\n",
    "#         available = available.find(\"span\").string.strip()\n",
    "\n",
    "#     except AttributeError:\n",
    "#         available = \"Not Available\"\t\n",
    "\n",
    "#     return available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "50111395",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "    # The webpage URL\n",
    "    URL = \"https://www.flipkart.com/clothing-and-accessories/dresses-and-gown/dress/women-dress/pr?sid=clo,odx,maj,jhy&otracker=categorytree&otracker=nmenu_sub_Women_0_Dresses\"\n",
    "\n",
    "    # HTTP Request\n",
    "    webpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "    # Soup Object containing all data\n",
    "    soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "    # Fetch links as List of Tag Objects\n",
    "    links = soup.find_all('a', attrs={'class': 'WKTcLC'})\n",
    "    \n",
    "    # Store the links\n",
    "    links_list = []\n",
    "\n",
    "    # Loop for extracting links from Tag Objects\n",
    "    for link in links:\n",
    "            links_list.append(link.get('href'))\n",
    "\n",
    "    d = {\"title\":[], \"price\":[], \"rating\":[], \"review_count\":[],\"offer_percent\":[], \"flipkart_assured\":[]}\n",
    "    \n",
    "    # Loop for extracting product details from each link \n",
    "    for link in links_list:\n",
    "        new_webpage = requests.get(\"https://flipkart.com\" + link, headers=HEADERS)\n",
    "\n",
    "        new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "        # Function calls to display all necessary product information\n",
    "        d['title'].append(get_title(new_soup))\n",
    "        d['price'].append(get_price(new_soup))\n",
    "        d['rating'].append(get_rating(new_soup))\n",
    "        d['review_count'].append(get_review_count(new_soup))\n",
    "        d['offer_percent'].append(get_offer_per(new_soup))\n",
    "        d['flipkart_assured'].append(flipkart_assured(new_soup))\n",
    "        \n",
    "\n",
    "    \n",
    "    flipkart_df = pd.DataFrame.from_dict(d)\n",
    "    flipkart_df['title'].replace('', np.nan, inplace=True)\n",
    "    flipkart_df = flipkart_df.dropna(subset=['title'])\n",
    "    flipkart_df.to_csv(\"flipkart_data.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "281a163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRAPE MULTIPLE PAGES\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # add your user agent \n",
    "    HEADERS = ({'User-Agent':'', 'Accept-Language': 'en-US, en;q=0.5'}) \n",
    "\n",
    "    # The webpage URL\n",
    "    BASE_URL = \"https://www.flipkart.com/clothing-and-accessories/dresses-and-gown/dress/women-dress/pr?sid=clo,odx,maj,jhy&otracker=categorytree&otracker=nmenu_sub_Women_0_Dresses\"\n",
    "\n",
    "    \n",
    "    def scrape_page(url):\n",
    "    \n",
    "        # HTTP Request\n",
    "        webpage = requests.get(BASE_URL, headers=HEADERS)\n",
    "\n",
    "        # Soup Object containing all data\n",
    "        soup = BeautifulSoup(webpage.content, \"html.parser\")\n",
    "\n",
    "        # Fetch links as List of Tag Objects\n",
    "        links = soup.find_all('a', attrs={'class': 'WKTcLC'})\n",
    "        \n",
    "        d = {\"title\":[], \"price\":[], \"rating\":[], \"review_count\":[],\"offer_percent\":[], \"flipkart_assured\":[]}\n",
    "        \n",
    "\n",
    "        # Loop for extracting links from Tag Objects\n",
    "        for link in links:\n",
    "            link_url = \"https://flipkart.com\" + link.get('href')\n",
    "            new_webpage = requests.get(link_url, headers=HEADERS)\n",
    "            new_soup = BeautifulSoup(new_webpage.content, \"html.parser\")\n",
    "\n",
    "            # Function calls to display all necessary product information\n",
    "            d['title'].append(get_title(new_soup))\n",
    "            d['price'].append(get_price(new_soup))\n",
    "            d['rating'].append(get_rating(new_soup))\n",
    "            d['review_count'].append(get_review_count(new_soup))\n",
    "            d['offer_percent'].append(get_offer_per(new_soup))\n",
    "            d['flipkart_assured'].append(flipkart_assured(new_soup))    \n",
    "    \n",
    "        return d\n",
    "    \n",
    "    # Scraping data from multiple pages\n",
    "    all_data = {\"title\": [], \"price\": [], \"rating\": [], \"review_count\": [], \"offer_percent\": [], \"flipkart_assured\": []}\n",
    "    for page_number in range(1, 6):  # Assuming you want to scrape data from 5 pages\n",
    "        url = f\"{BASE_URL}&page={page_number}\"\n",
    "        page_data = scrape_page(url)\n",
    "        for key in all_data.keys():\n",
    "            all_data[key].extend(page_data[key])\n",
    "        \n",
    "\n",
    "    \n",
    "    flipkart_df = pd.DataFrame.from_dict(all_data)\n",
    "    flipkart_df['title'].replace('', np.nan, inplace=True)\n",
    "    flipkart_df = flipkart_df.dropna(subset=['title'])\n",
    "    flipkart_df.to_csv(\"flipkart_data_m.csv\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9ca381",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
